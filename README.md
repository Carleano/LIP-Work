In over 60 years of space exploration, NASA has produced a vast library of images that not only inform the public about the universe they live in but also provide an unmatched level of awe and wonder.  However, people who are blind or visually traditionally have had no access to NASA's visual record of exploration and have been left out of most of astronomy and astrophysics.  In this project, we plan to address this lack of access by using advanced image filtering techniques to partition images into component, physically meaningful segments.  These segments will then be quantified by a representative haptic and auditory layer that can be accessed on mobile devices using a touch screen allowing for the independent investigation of the images via sight, sound, and touch. The haptic and auditory layer will be signed to be qualitatively as well as quantitatively descriptive, allowing users to enjoy as well as learn from the encoded images.
